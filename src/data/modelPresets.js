// Пресеты для популярных моделей
export const MODEL_PRESETS = {
  "llama2-7b": {
    "name": "Llama 2 7B",
    "params": 7,
    "tokensPerSec": null,
    "description": "Llama 2 7B — это открытая базовая языковая модель, выпущенная Meta AI в 2023 году, с 7 миллиардами параметров. Она является частью семейства предобученных и чат-моделей Llama 2, разработанных для демократизации доступа к ИИ. Llama 2 7B предлагает контекстное окно в 4096 токенов, что вдвое больше, чем у LLaMA 1, и была обучена на 40% большем объеме данных, чем LLaMA 1, что улучшило ее знания и согласованность. Однако, как самая маленькая модель в серии, ее производительность уступает более крупным моделям в сложных задачах рассуждения и программирования.",
    "recommended": false,
    "supports_tool_calls": false,
    "developer": "Meta AI",
    "context": "4096 tokens (4k context)",
    "contextValue": 4096,
    "license": "Llama 2 Community License (custom non-OSI license for research & commercial use with some restrictions)",
    "moe": "No",
    "optimizations": "RoPE, SwiGLU activation, RMSNorm; full attention (no GQA at 7B scale)",
    "multimodality": "Text"
  },
  "llama2-13b": {
    "name": "Llama 2 13B",
    "params": 13,
    "tokensPerSec": null,
    "description": "Llama 2 13B — это модель с 13 миллиардами параметров в серии Llama 2 от Meta. Как и вариант 7B, она доступна для бесплатного исследовательского и коммерческого использования и поддерживает контекстное окно в 4096 токенов. Модель 13B обеспечивает лучший баланс возможностей и эффективности, давая более согласованные и точные ответы, чем 7B, во многих задачах. Она была предобучена на том же расширенном корпусе данных, с применением дообучения с подкреплением (reinforcement learning) в чат-версиях для улучшения согласованности. В целом, Llama 2 13B предлагает более сильные возможности рассуждения и программирования, чем 7B, хотя все еще уступает самым большим моделям в очень сложных задачах.",
    "recommended": false,
    "supports_tool_calls": false,
    "developer": "Meta AI",
    "context": "4096 tokens",
    "contextValue": 4096,
    "license": "Llama 2 Community License (Meta's proprietary open license)",
    "moe": "No",
    "optimizations": "RoPE, SwiGLU, RMSNorm; no MoE or GQA (dense Transformer architecture)",
    "multimodality": "Text"
  },
  "llama2-70b": {
    "name": "Llama 2 70B",
    "params": 70,
    "tokensPerSec": null,
    "description": "Llama 2 70B — самая большая модель в семействе Llama 2, с 70 миллиардами параметров. Это была передовая открытая модель 2023 года, достигшая значительно лучших результатов, чем Llama 1, и приближающаяся к уровню производительности GPT-3.5 на многих бенчмарках. Llama 2 70B использует контекстное окно в 4096 токенов и была дообучена с использованием обратной связи от людей для согласованности, что делает ее способной к диалогу. Архитектурно она представила Grouped Query Attention (GQA) для эффективности, как используется в 70B модели Llama 2. Хотя более поздние модели превзошли ее, Llama 2 70B остается мощной моделью с открытым исходным кодом для сложных языковых задач.",
    "recommended": false,
    "supports_tool_calls": false,
    "developer": "Meta AI",
    "context": "4096 tokens",
    "contextValue": 4096,
    "license": "Llama 2 Community License",
    "moe": "No",
    "optimizations": "RoPE, SwiGLU, RMSNorm, and GQA (applied at 70B scale for efficiency)",
    "multimodality": "Text"
  },
  "llama3-8b": {
    "name": "Llama 3.1 8B",
    "params": 8,
    "tokensPerSec": null,
    "description": "Llama 3.1 8B — это модель с 8 миллиардами параметров в третьем поколении серии Llama от Meta (выпущена в 2024 году). Несмотря на свой относительно небольшой размер, Llama 3.1 8B получила значительные улучшения производительности по сравнению с Llama 2 благодаря обучению на ~15 триллионах токенов разнообразных данных и улучшенной настройке модели. Она поддерживает значительно расширенную длину контекста до 128 000 токенов, что позволяет ей обрабатывать очень длинные входные данные. Модели Llama 3.1 представили улучшенные многоязычные возможности и функцию «расширенного использования инструментов» для лучшей интеграции с внешними инструментами. В целом, Llama 3.1 8B предлагает сильную общую и логическую производительность для своего размера, значительно сокращая разрыв с более крупными предшественниками.",
    "recommended": true,
    "supports_tool_calls": true,
    "developer": "Meta AI",
    "context": "128000 tokens (128k context window)",
    "contextValue": 128000,
    "license": "Llama 3.1 Community License (custom commercial license)",
    "moe": "No",
    "optimizations": "Transformer arch. w/ RoPE (128k), SwiGLU, RMSNorm, improved training efficiency; GQA applied in larger variants",
    "multimodality": "Text (multilingual)"
  },
  "llama3-70b": {
    "name": "Llama 3.1 70B",
    "params": 70,
    "tokensPerSec": null,
    "description": "Llama 3.1 70B — это модель с 70 миллиардами параметров в коллекции Llama 3.1 от Meta, служащая обновленной версией Llama 2 70B с расширенными возможностями. Она выигрывает от огромного обучающего корпуса Llama 3 (~15 трлн токенов) и продвинутой дообучения, что приводит к улучшениям в рассуждении, программировании и многоязычном понимании. Как и все модели Llama 3.1, 70B поддерживает контекстное окно до 128 тыс. токенов — намного больше, чем 4 тыс. у Llama 2 — что позволяет ей обрабатывать очень длинные документы или диалоги. Она также имеет расширенные возможности использования инструментов и улучшения безопасности. Llama 3.1 70B обеспечивает передовую производительность открытых моделей (примерно 2024 год), будучи при этом проще в развертывании, чем самая большая модель 405B.",
    "recommended": true,
    "supports_tool_calls": true,
    "developer": "Meta AI",
    "context": "128000 tokens",
    "contextValue": 128000,
    "license": "Llama 3.1 Community License",
    "moe": "No",
    "optimizations": "RoPE (128k), SwiGLU, RMSNorm, GQA (for efficiency), other Llama 3 training improvements",
    "multimodality": "Text"
  },
  "llama3-405b": {
    "name": "Llama 3.1 405B",
    "params": 405,
    "tokensPerSec": null,
    "description": "Llama 3.1 405B — это передовая открытая модель от Meta с поразительными 405 миллиардами параметров. Выпущенная в июле 2024 года, это самая большая и способная модель Llama, нацеленная на производительность класса GPT-4. Дообученная на инструкциях Llama 3.1 405B достигает беспрецедентного паритета с ведущими проприетарными LLM, превосходя многие бенчмарки и конкурентные оценки. Она поддерживает контекстное окно до 128 тыс. токенов, позволяя обрабатывать чрезвычайно длинные входные данные. Несмотря на свой размер, Meta оптимизировала обучение с помощью обширного кластера GPU (39.3 млн GPU-часов на H100). Llama 3.1 405B считается передовой открытой базовой моделью, хотя для ее запуска требуются огромные вычислительные ресурсы.",
    "recommended": true,
    "supports_tool_calls": true,
    "developer": "Meta AI",
    "context": "128000 tokens",
    "contextValue": 128000,
    "license": "Llama 3.1 Community License",
    "moe": "No",
    "optimizations": "Dense Transformer (no MoE) w/ RoPE (128k), SwiGLU, RMSNorm; fine-tuned for alignment; optional CoT thinking mode",
    "multimodality": "Text"
  },
  "mixtral-8x7b": {
    "name": "Mixtral 8×7B",
    "params": 46.7,
    "tokensPerSec": null,
    "description": "Mixtral 8×7B — это разреженная языковая модель Mixture-of-Experts (SMoE), выпущенная Mistral AI в декабре 2023 года. Она имеет ту же базовую архитектуру, что и Mistral 7B, но каждый слой Transformer содержит 8 модулей-экспертов прямой связи (feedforward). Для каждого токена маршрутизатор выбирает 2 из 8 экспертов для его обработки, поэтому, хотя Mixtral имеет 46.7 млрд общих параметров, только ~12.9 млрд активны для каждого токена. Эта конструкция дает Mixtral 8×7B качество гораздо более крупной модели: она соответствует или превосходит Llama 2 70B и GPT-3.5 на большинстве бенчмарков, работая при этом примерно в 6 раз быстрее, чем Llama 2 70B. Она обрабатывает контекст до 32 тыс. токенов и особенно преуспевает в математике, генерации кода и многоязычном понимании. Mixtral имеет открытые веса и лицензируется под Apache 2.0.",
    "recommended": true,
    "supports_tool_calls": false,
    "developer": "Mistral AI",
    "context": "32000 tokens",
    "contextValue": 32000,
    "license": "Apache 2.0",
    "moe": "Yes (Sparse MoE with 8 experts per layer, 2 activated for each token)",
    "optimizations": "RoPE (32k); MoE arch.; BF16 weights; Mistral 7B Transformer backbone",
    "multimodality": "Text"
  },
  "qwen-72b": {
    "name": "Qwen-72B",
    "params": 72,
    "tokensPerSec": null,
    "description": "Qwen-72B — самая большая модель первого поколения открытых LLM Qwen (Tongyi Qianwen) от Alibaba Cloud, представленная в 2023 году. Она содержит ~72.7 млрд параметров и была обучена на ~3 триллионах токенов многоязычных данных (сильно двуязычная англо-китайская). Qwen-72B поддерживает контекстное окно в 32 тыс. токенов с помощью ротационных позиционных эмбеддингов и расширенного предобучения, что позволяет ей обрабатывать длинные входные данные. Она достигла конкурентоспособной производительности по сравнению с современными моделями, такими как Llama 2 70B, GPT-3.5 и даже GPT-4 на некоторых бенчмарках. Примечательно, что Qwen включает встроенные возможности использования инструментов (с системной подсказкой и форматом вызова функций) и была выпущена с разрешительной открытой лицензией в Китае. В целом, Qwen-72B была передовой открытой моделью в конце 2023 года, особенно сильной в китайском языке и многоходовых диалогах.",
    "recommended": false,
    "supports_tool_calls": true,
    "developer": "Alibaba Cloud (DAMO/Qwen Team)",
    "context": "32000 tokens",
    "contextValue": 32000,
    "license": "Tongyi Qianwen Open License (custom Alibaba license)",
    "moe": "No",
    "optimizations": "Transformer w/ RoPE (32k), SwiGLU, RMSNorm, GQA (memory efficiency), attention QKV bias",
    "multimodality": "Text"
  },
  "qwen2-72b": {
    "name": "Qwen2-72B",
    "params": 72,
    "tokensPerSec": null,
    "description": "Qwen 2-72B — это флагманская плотная модель второго поколения серии Qwen от Alibaba (анонсирована в середине 2024 года). Она включает многочисленные улучшения по сравнению с Qwen-1: расширенное многоязычное обучение (данные на 27 дополнительных языках) и значительно улучшенную производительность в кодировании, математике, рассуждении и следовании инструкциям. Модели Qwen2 поддерживают чрезвычайно длинные контекстные окна – базовые модели обучаются на 32 тыс. токенах и демонстрируют экстраполяцию на основе перплексии до ~128 тыс., в то время как выровненные чат-модели явно расширены для обработки входных данных до 128 тыс. токенов. Архитектура Qwen2 использует оптимизации Transformer, такие как SwiGLU и смещение QKV внимания, и применяет Grouped Query Attention ко всем размерам моделей для более быстрого вывода. В целом, Qwen2-72B обычно превосходит большинство открытых моделей своей эпохи и конкурентоспособна с некоторыми проприетарными моделями по широкому спектру бенчмарков.",
    "recommended": false,
    "supports_tool_calls": true,
    "developer": "Alibaba Cloud",
    "context": "32000 tokens (base pretrained) / 128000 tokens (chat fine-tuned)",
    "contextValue": 128000,
    "license": "Tongyi Qianwen License for 72B (Qwen2's smaller models are Apache-2.0)",
    "moe": "No (a separate MoE variant, 57B-A14B, exists in Qwen2 series)",
    "optimizations": "RoPE (32k pretrain, 128k w/ YaRN); GQA (all sizes); SwiGLU; RMSNorm; improved multilingual tokenizer",
    "multimodality": "Text"
  },
  "qwen2.5-72b": {
    "name": "Qwen2.5-72B",
    "params": 72,
    "tokensPerSec": null,
    "description": "Qwen2.5-72B — это обновленная 72B модель из серии Qwen от Alibaba (конец 2024 года), предлагающая существенный прирост знаний и навыков по сравнению с Qwen2. Она была предобучена на данных объемом до 18 триллионов токенов и демонстрирует значительно улучшенные способности к кодированию и математике (через специализированные компоненты-эксперты), более сильное следование инструкциям (например, обработка таблиц, вывод JSON, ролевые игры) и может генерировать очень длинные выходные данные (далеко за пределы 8 тыс. токенов). Qwen2.5 нативно поддерживает контекстные окна в 131 072 токена. Архитектурно она использует Transformer с RoPE, SwiGLU, RMSNorm, смещением QKV и Grouped Query Attention (64 головки запроса / 8 групп KV). В целом, Qwen2.5-72B обеспечила передовую производительность открытых моделей на рубеже 2025 года, соперничая с более крупными моделями при сохранении эффективного вывода.",
    "recommended": true,
    "supports_tool_calls": true,
    "developer": "Alibaba Cloud",
    "context": "131072 tokens",
    "contextValue": 131072,
    "license": "Tongyi Qianwen Open License (Qwen2.5 series)",
    "moe": "No",
    "optimizations": "Transformer w/ RoPE (128k), SwiGLU, RMSNorm, QKV bias; GQA (80L, 64Q/8KV); extensive fine-tuning (code/math)",
    "multimodality": "Text"
  },
  "qwen3-235b": {
    "name": "Qwen3-235B-A22B",
    "params": 235,
    "tokensPerSec": null,
    "description": "Qwen3-235B-A22B — это флагманская модель третьего поколения семейства Qwen от Alibaba, выпущенная в апреле 2025 года. Это гибридная модель Mixture-of-Experts с 235 млрд общих параметров и ~22 млрд активируемых на токен (128 экспертов, 8 выбираемых на токен). Qwen3-235B достигает высочайшей производительности в задачах кодирования, математики и общих языковых задачах – сообщается, что она соответствует или превосходит другие ведущие открытые модели, такие как DeepSeek-R1, и конкурирует с лучшими закрытыми моделями (например, от OpenAI и Google). Уникально то, что модели Qwen3 вводят переключаемый «режим мышления» для сложных рассуждений, позволяя модели внутренне дольше размышлять ценой задержки. Qwen3-235B поддерживает контекст в 32 тыс. токенов нативно и до 128 тыс. токенов с расширением YaRN. Она имеет открытые веса под лицензией Apache 2.0 и представляет собой одну из самых мощных общедоступных LLM по состоянию на 2025 год.",
    "recommended": true,
    "supports_tool_calls": true,
    "developer": "Alibaba Cloud (Qwen 3 Team)",
    "context": "131072 tokens (32k native, 128k with YaRN)",
    "contextValue": 131072,
    "license": "Apache 2.0",
    "moe": "Yes (Mixture-of-Experts: 235B total, ~22B active per token – 128 experts with 8 active)",
    "optimizations": "Hybrid reasoning (thinking/fast modes); RoPE w/ YaRN (128k); GQA (64Q/4KV); MoE arch. w/ dynamic routing",
    "multimodality": "Text"
  },
  "deepseek-67b": {
    "name": "DeepSeek 67B",
    "params": 67,
    "tokensPerSec": null,
    "description": "DeepSeek 67B — это базовая языковая модель с 67 миллиардами параметров, разработанная Hangzhou DeepSeek AI (Китай) в 2024 году. Она была обучена с нуля на огромном наборе данных в 2 триллиона токенов, включающем как английский, так и китайский текст, с целью создания сильной двуязычной основы. DeepSeek 67B использует архитектуру Transformer, подобную LLaMA, с Grouped-Query Attention (GQA), чтобы обеспечить больший масштаб и более быстрый вывод. В оценках она превосходит Llama 2 70B в сложных математических и кодовых задачах, иллюстрируя преимущества ее высококачественных данных и стабильности обучения. Контекстное окно модели по умолчанию составляет 4096 токенов (с расширенным контекстом до 128 тыс. токенов, доступным в дообученной чат-версии), и она была выпущена с открытыми весами для исследовательского и коммерческого использования.",
    "recommended": true,
    "supports_tool_calls": false,
    "developer": "DeepSeek AI",
    "context": "4096 tokens (extended up to 128k in DeepSeek-67B-Chat)",
    "contextValue": 4096,
    "license": "MIT License (open-weight)",
    "moe": "No",
    "optimizations": "GQA (efficiency); RoPE; RMSNorm; FP16/BF16 training",
    "multimodality": "Text"
  },
  "deepseek-v3-671b": {
    "name": "DeepSeek-V3 671B",
    "params": 671,
    "tokensPerSec": null,
    "description": "DeepSeek-V3 — это модель Sparse MoE с 671 млрд параметров (с ~37 млрд параметров, активируемых на токен), выпущенная DeepSeek AI в конце 2024 года. Она вводит передовые архитектурные инновации для максимизации эффективности в масштабе: Multi-Head Latent Attention (которая сжимает ключи/значения внимания для уменьшения памяти) и механизм DeepSeekMoE (128 экспертов с улучшенной балансировкой нагрузки, устраняющий вспомогательные потери). DeepSeek-V3 была предобучена на 14.8 триллионах высококачественных токенов, а затем дообучена (включая RL) для полной реализации своих возможностей. В результате получилась модель, которая превосходит все предыдущие модели с открытым исходным кодом и достигает производительности, сравнимой с ведущими закрытыми моделями, такими как GPT-4, при этом обученная за долю стоимости (всего ~2.8 млн GPU-часов H800). Она поддерживает очень длинные контекстные окна (протестировано до 128 тыс. токенов с улучшенной обработкой RoPE). Выпуск DeepSeek-V3 (под лицензией MIT) произвел фурор в отрасли, продемонстрировав, что ИИ передового уровня можно достичь с относительно низким бюджетом.",
    "recommended": true,
    "supports_tool_calls": false,
    "developer": "DeepSeek AI",
    "context": "128000 tokens (long-context capable)",
    "contextValue": 128000,
    "license": "MIT License",
    "moe": "Yes (Sparse MoE model with 671B total params, ~37B active per token)",
    "optimizations": "MHLA (compressed KV); DeepSeekMoE arch. (aux-loss-free routing); FP8 training; multi-token prediction",
    "multimodality": "Text"
  },
  "yi-34b": {
    "name": "Yi-34B",
    "params": 34,
    "tokensPerSec": null,
    "description": "Yi-34B — это открытая базовая модель с 34 млрд параметров, выпущенная 01.AI (ZeroOne) в 2024 году. Она была предобучена на тщательно отфильтрованном корпусе из 3 триллионов токенов, охватывающем английский и китайский языки, что привело к передовой производительности среди открытых моделей среднего размера (она занимает лидирующие позиции в оценках, таких как MMLU и рассуждениях на основе здравого смысла). Yi-34B основывается на архитектуре LLaMA с несколькими улучшениями: она применяет Grouped Query Attention даже в масштабе 34B (следуя подходу LLaMA 2's 70B approach) для снижения использования памяти без потери производительности и использует функции активации SwiGLU для повышения эффективности обучения. Нативное контекстное окно модели составляет 4096 токенов, но доступна расширенная версия (Yi-34B-200K), поддерживающая контекст до 200 тыс. токенов благодаря продолжению предобучения на длинных последовательностях. Yi-34B выпущена под лицензией Apache 2.0 и быстро стала популярной открытой моделью для исследований в области NLP как на английском, так и на китайском языках.",
    "recommended": true,
    "supports_tool_calls": false,
    "developer": "01.AI (ZeroOne)",
    "context": "4096 tokens (200k in extended version)",
    "contextValue": 4096,
    "license": "Apache 2.0",
    "moe": "No",
    "optimizations": "LLaMA Transformer w/ GQA (56Q/8KV); SwiGLU (FFN); RoPE (mod. base freq. for 200k context)",
    "multimodality": "Text"
  },
  "qwq-32b": {
    "name": "QwQ-32B",
    "params": 32,
    "tokensPerSec": null,
    "description": "QwQ-32B — это модель с 32 млрд параметров, ориентированная на рассуждения, разработанная командой Qwen из Alibaba (анонсирована в марте 2025 года). Она была создана путем применения крупномасштабного обучения с подкреплением (RL) на сильной базовой модели, что позволило QwQ-32B достичь производительности, сравнимой с DeepSeek-R1 (модель MoE 671B), несмотря на ее гораздо меньший размер. В частности, QwQ-32B преуспевает в сложных математических рассуждениях и задачах кодирования благодаря дообучению RL с верификатором точности для математики и вознаграждениями за выполнение кода. Модель также дополнена агентскими возможностями: она может вызывать инструменты и адаптировать свои рассуждения на основе обратной связи, интегрируя форму выполнения цепочки мыслей (chain-of-thought). QwQ-32B поддерживает чрезвычайно длинные контексты (полные 131 072 токена с YaRN для входных данных свыше 8 тыс.) и выпущена под лицензией Apache 2.0. Она считается одной из лучших «маленьких» моделей с открытым исходным кодом для сложных задач рассуждения в 2025 году.",
    "recommended": true,
    "supports_tool_calls": true,
    "developer": "Alibaba (Qwen Team)",
    "context": "131072 tokens (8192 tokens without YaRN, up to 131072 with YaRN)",
    "contextValue": 131072,
    "license": "Apache 2.0",
    "moe": "No",
    "optimizations": "Transformer w/ RoPE (YaRN); SwiGLU, RMSNorm, QKV bias; GQA (40Q/8KV); extensive RL fine-tuning (reasoning/tool use)",
    "multimodality": "Text"
  }
}
