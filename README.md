# Калькулятор ЦОД для GenAI Моделей

**Автор:** Ed Cherednik

Простой веб-инструмент для расчета и оценки затрат на инфраструктуру (CapEx, OpEx, TCO) и требуемых ресурсов (GPU, серверы, мощность, сеть, хранилище, RAM) для развертывания и эксплуатации больших языковых моделей (LLM) и других моделей генеративного ИИ.

## Возможности

*   **Гибкая настройка:** Задавайте параметры LLM (размер, точность), нагрузку (пользователи, время ответа), конфигурацию оборудования (GPU, серверы) и стоимость ресурсов ЦОД.
*   **Пресеты:** Используйте готовые пресеты для популярных моделей (LLaMA, Qwen, Mixtral и др.), GPU (NVIDIA, AMD, Intel, Huawei) и серверных платформ (Dell, HPE, Supermicro).
*   **Детальные результаты:** Получайте расчеты по требуемому количеству GPU, серверов, общей мощности, стоимости CapEx (с разбивкой по компонентам), OpEx (энергия, обслуживание) и TCO за 5 лет.
*   **Технический отчет:** Формирование текстового отчета с ключевыми выводами и рекомендациями по оптимизации конфигурации.
*   **Аналитика и KPI:** Визуальное представление ключевых метрик эффективности (стоимость/токен, производительность/пользователь, утилизация GPU, структура TCO) для помощи в принятии решений.
*   **Современный интерфейс:** Построен с использованием Ant Design.

## Технологический стек

*   **Фреймворк:** React
*   **Сборка:** Vite
*   **UI Библиотека:** Ant Design
*   **Язык:** JavaScript (JSX)

## Начало работы

### Требования

*   Node.js (версия 18+ рекомендуется)
*   npm (обычно идет с Node.js)

### Установка

1.  Клонируйте репозиторий:
    ```bash
    git clone <repository-url>
    cd <repository-directory>
    ```
2.  Установите зависимости:
    ```bash
    npm install
    ```

### Запуск в режиме разработки

Для запуска локального сервера разработки с hot-reload:

```bash
npm run dev
```

Откройте браузер и перейдите по адресу, указанному в терминале (обычно `http://localhost:5173` или следующий свободный порт).

### Сборка для Production

Для создания оптимизированной сборки приложения для развертывания:

```bash
npm run build
```

Собранные файлы будут находиться в папке `dist`.

### Предпросмотр Production сборки

Для локального запуска собранной версии:

```bash
npm run preview
```

## Справочник по открытым LLM (большим языковым моделям)

Введение

Открытые большие языковые модели (LLM) – это модели на основе трансформеров с количеством параметров от нескольких миллиардов и выше, свободно доступные исследователям и разработчикам. Они обучены предсказывать текст по предыдущему контексту и способны генерировать связанный ответ на вводную подсказку. В этом справочнике рассмотрены технические характеристики популярных открытых LLM: их архитектура, типы слоёв, механизмы внимания, форматы числового представления данных, используемые оптимизаторы, схемы позиционного кодирования, а также требования к оборудованию для обучения и инференса. Также упомянуты методы сжатия и дообучения – квантование (quantization), разреженность (sparsity), адаптация с пониженным рангом (LoRA) и смесь экспертов (MoE), и насколько они поддерживаются каждой моделью. Модели представлены с указанием числа параметров, особенностей pre-training (предобучения) и fine-tuning (дообучения), а также ключевых технических отличий.

Основные понятия: Все рассматриваемые модели являются авто-регрессионными (decoder-only) трансформерами, генерирующими текст токен за токен. Они состоят из многослойных блоков self-attention (самовнимания) и последующих полносвязных слоёв (FFN). Ключевые архитектурные различия между моделями заключаются в методах нормализации (например, LayerNorm vs RMSNorm), активациях в FFN (например, GeLU vs SwiGLU), разновидностях механизма внимания (стандартное многоголовое, многозапросное, группированное и пр.), а также в способе задания позиционной информации (например, абсолютное, Rotary, ALiBi и т.д.). При обучении LLM обычно используют пониженные форматы точности чисел – FP16 (half-precision) или BF16 – чтобы сократить память и ускорить вычисления ￼ ￼. Для инференса моделей применяют квантование до 8 или 4 бит, что позволяет запускать даже 175-миллиардные модели на одном сервере без потери качества вывода ￼ (например, с помощью метода LLM.int8() для 8-битных матриц в Transformers ￼). Ниже представлено подробное описание открытых моделей LLM.

### Форматы представления данных и оптимизации

**FP16 и BF16:** Почти все современные LLM обучаются с микшированной точностью – весовые параметры хранятся в FP32, но вычисления выполняются в 16-битных форматах. Стандартный FP16 (half precision) имеет ограниченный диапазон экспоненты, из-за чего большие градиенты могут переполняться. Формат BF16 (bfloat16) решает эту проблему, имея тот же диапазон экспоненты, что и FP32, но укороченную мантиссу ￼. BF16 позволяет избежать overflow-ошибок FP16, сохраняя более стабильное обучение, при этом по скорости и памяти BF16 эквивалентен FP16 ￼. Поэтому многие крупные модели (например, BLOOM) тренировались в режиме BF16 ￼. В инференсе же зачастую достаточно FP16.

**INT8 и INT4 квантование:** Для ускорения инференса и уменьшения требований VRAM применяют квантование весов до 8-битного или 4-битного целочисленного формата. Современные методы, такие как LLM.int8(), позволяют выполнять матричные умножения в 8 бит без заметного ухудшения качества, за счёт раздельной обработки выбросов (outliers) с более высокой точностью ￼. Было показано, что 175-миллиардную модель (OPT-175B/BLOOM) можно запустить в 8-битном режиме без потери качества ￼ – это сокращает потребление памяти вдвое. Ещё более агрессивно 4-битное квантование (например, GPTQ, либо QLoRA для последующего дообучения): оно даёт ~4х экономию памяти. Например, для модели на 130 млрд параметров (GLM-130B) переход с FP16 (~320 ГБ на инференс с учётом активаций) до 4-бит уменьшает требование памяти до ~88 ГБ ￼ ￼. Квантование позволяет загружать большие модели на 1–2 GPU, ценой небольшого падения точности (которое минимально при правильной калибровке). В целом, INT8 является стандартным для ускорения инференса, а INT4 – для максимально возможной экономии памяти, например при дообучении с QLoRA ￼.

**Optimizers (оптимизаторы):** Для предобучения LLM обычно используют AdamW – вариант Adam с L2-регуляризацией как весовым распадом. AdamW хорошо зарекомендован на больших моделях ￼. Например, исходное обучение LLaMA проводилось AdamW с определёнными гиперпараметрами ￼. В некоторых работах для супербольших моделей применяли оптимизатор LAMB (более стабильный на больших батчах) или Adafactor (экономящий память на аккумулирование моментов). Однако в открытых проектах (LLaMA, BLOOM, Falcon и др.) основным остаётся AdamW. Также для ускорения и снижения памяти применяются 8-битные оптимизаторы (например, в библиотеке BitsAndBytes), позволяющие хранить моменты в 8 бит без ухудшения качества. К примеру, BLOOM тренировали с BF16 вычислениями и хранением основных весов, а состояния оптимизатора (моменты Adam) – в 32-бит с_sharding_-разбиением по узлам ￼.

**LoRA (Low-Rank Adaptation):** Почти все открытые LLM поддерживают эффективное дообучение методом LoRA. LoRA замораживает исходные веса модели и добавляет к каждому весовому тензору пары малых ранговых матриц-адаптеров, которые обучаются на новой задаче ￼. Это радикально снижает число обучаемых параметров (на порядки), позволяя обучать большую модель на одной-двух видеокартах. Примечательно, что LoRA-дообучение зачастую не уступает полному fine-tuning и даже избегает катастрофического забывания исходных навыков модели ￼. В 2023 году появились инструменты QLoRA – комбинация 4-битного квантования и LoRA-адаптеров, что позволило даже 65B модели fine-tune на одной GPU 48 ГБ. Благодаря открытости моделей LLaMA, Falcon и др., сообществом были созданы многочисленные LoRA-версии (например, Stanford Alpaca, Vicuna и т.д. на базе LLaMA). Ниже, для каждой модели, отмечены примеры поддержки LoRA и др. методов.

**MoE (Mixture-of-Experts):** Большинство открытых моделей являются плотными (dense) трансформерами, где все параметры задействованы при выводе. Однако существуют открытые реализации архитектуры смеси экспертов, где часть слоёв содержит несколько экспертных модулей, и для каждого входа активируется только один (или несколько) экспертов, что существенно увеличивает общее число параметров при сопоставимых затратах на инференс. Пример – модель Phi-3.5-MoE от Microsoft: она имеет 41,9 млрд параметров с MoE-слоем, активирующим разные подсетки параметров в зависимости от входа ￼. Такая модель обучена на 4,9 трлн токенов и превосходит плотные модели большего размера по ряду задач ￼. Хотя подавляющее большинство популярных LLM (LLaMA, Falcon, BLOOM и др.) – плотные, метод MoE развивается как способ увеличить ёмкость без пропорционального роста затрат, и вероятно будет интегрироваться в новые открытые модели.

### LLaMA (Large Language Model Meta AI)

**Разработчик:** Meta AI (Fair). **Дата выпуска:** февраль 2023 (LLaMA 1) и июль 2023 (LLaMA 2).
**Размеры модели:** 7B, 13B, 33B, 65B параметров (LLaMA 1); LLaMA 2 – 7B, 13B, 70B параметров.
**Архитектура:** Авто-регрессивный трансформер (decoder-only). Применена система улучшений по сравнению с оригинальным GPT-3-подобным трансформером: предварительная RMS-нормализация вместо пост-слойной нормализации, функция активации SwiGLU (gated SiLU) в FFN, и Multi-Query Attention (MQA) – многозапросное внимание, где для всех голов внимания используются общие ключи и значения ￼. Позиционное кодирование – ротари-эмбеддинги (RoPE), которые вводятся в проекции ключей/значений внимания, обеспечивая периодическое смещение фаз для кодирования позиций ￼. Оптимизатор при обучении – AdamW ￼. Все слои трансформера лишены смещений (bias) для упрощения и меньшего числа параметров. LLaMA использует пре-нормализацию: RMSNorm применяется в начале каждого блока внимания и FFN. Контекстное окно LLaMA 1 – 2048 токенов. LLaMA 2 получила увеличение контекста до 4096 токенов (в 2 раза больше) ￼. Кроме того, в LLaMA 2 для моделей 70B введён Grouped-Query Attention (GQA) – группировка голов внимания: например, 70B модель имеет 8 групп KV-голов на 32 головы запроса, что сокращает размер кэша внимания и ускоряет декодирование ￼. В версиях 7B и 13B LLaMA2 сохранилось MQA, а 70B перешла на GQA.

**Предобучение:** LLaMA 1 обучена на 1.0–1.4 трлн токенов текста из общедоступных источников (C4, GitHub, Wikipedia, книги, арXiv, StackExchange и др. в общей сложности ~4 ТБ данных ￼). Например, 7B версия обучалась на ~1 трлн токенов, 65B – на 1.4 трлн ￼. Это значительно больше объёма данных, чем у GPT-3 при сопоставимых параметрах, что обеспечило высокое качество. Обучение LLaMA проводилось на кластерe с 2048 GPU NVIDIA A100 80GB в течение примерно 21 дня (для 65B). LLaMA 2 обучалась на ~2 трлн токенов, включающих более современный и разнообразный корпус. Обе версии LLaMA использовали half-precision вычисления (FP16 или BF16) и распределённый оптимизатор ZeRO с диаграммами (для экономии памяти). Выпуск LLaMA 1 имел ограниченную некоммерческую лицензию (исходные веса предоставлялись по запросу ученым), но позже модель «утекла» и стала де-факто открытой. LLaMA 2 распространяется по более открытой лицензии (пермиссивной для коммерческого использования, кроме некоторых ограничений для модели 70B).

**Инференс и требования:** Благодаря сравнительно небольшим размерам, младшие версии LLaMA стали популярны для локального запуска. LLaMA 7B (7 млрд параметров) в FP16 требует ~14–16 ГБ VRAM для загрузки модели, поэтому может работать на одной видеокарте RTX 3090/4090. В 8-битном режиме (INT8) модель 7B помещается примерно в 8 ГБ VRAM, а при 4-битном квантовании – ~5–6 ГБ, позволяя запуск даже на некоторых GPU с 8 ГБ памяти. LLaMA 13B (13 млрд) – ~26 ГБ FP16 (около 20 ГБ в INT8). LLaMA 65B требует около 130 ГБ в FP16 только для весов ￼ (то есть минимум 2–3 GPU A100 80GB для инференса), а с 4-битным квантованием снижается до ~33 ГБ, что всё ещё превышает память одного 24-GB GPU, но уже помещается на паре высококлассных карт. Таким образом, 65B обычно запускают на 4×A100 40GB или 2×A100 80GB (либо на CPU с медленной скоростью). LLaMA 70B аналогична 65B по масштабам. Для fine-tuning LLaMA обычно применяют LoRA-методы: выпущено множество адаптаций, включая диалоговые (Alpaca, Vicuna и др.). Эти модели показали, что 13-миллиардная LLaMA способна по качеству на уровне приближаться к GPT-3 175B ￼, а 65B – соперничать с моделями-гигантами (Chinchilla-70B, PaLM-540B) ￼, при существенно меньшем размере. Это стало возможным благодаря сочетанию архитектурных улучшений и огромного корпуса данных.

**Fine-tuning и производные:** Meta выпустила для LLaMA 2 готовые дообученные версии для диалогов – LLaMA-2-Chat (7B, 13B, 70B), обученные с помощью SFT и RLHF. Они оптимизированы под разговорные задачи. Из-за ограничений лицензии LLaMA 1, сообществом были популярны инструкционные модели (например, Vicuna 13B) через LoRA, которые расширяют возможности LLaMA 1. LLaMA 2 официально разрешена для коммерческого использования, что стимулировало интеграцию её в приложения. Model LLaMA легко поддерживает LoRA: добавление адаптеров значительно уменьшает требования к памяти при fine-tuning (для 7B – достаточно GPU с 12–16 ГБ). Также LLaMA совместима с MoE-расширениями: есть исследования, добавляющие экспертные слои к LLaMA (например, проект AsymGQA и др.), но out-of-the-box LLaMA – плотная модель без MoE.

**OpenLLaMA (реплика):** Отдельно стоит упомянуть проект OpenLLaMA – это открытая реализация LLaMA, обученная сообществом с нуля на открытых данных. OpenLLaMA 7B и 13B были натренированы на базе RedPajama – открытой реконструкции датасета LLaMA (около 1.2 трлн токенов) ￼. Архитектурно OpenLLaMA идентична оригиналу (32 слоя для 7B, 40 слоёв для 13B, RoPE, RMSNorm, SwiGLU и т.д.), но веса выпущены под лицензией Apache 2.0 ￼, позволяя свободное коммерческое использование. Модели OpenLLaMA обучались на TPU (TPU-v4) с фреймворком EasyLM на JAX ￼. Качество OpenLLaMA 7B немного уступает LLaMA 1 7B, но открытая лицензия делает её привлекательной. Также анонсированы OpenLLaMA v2, комбинирующие датасет RedPajama с дополнительными источниками (RefinedWeb от Falcon) ￼. OpenLLaMA поддерживает те же техники оптимизации и дообучения, что и LLaMA. Требования к VRAM для инференса такие же (см. выше).

### Falcon

**Разработчик:** Technology Innovation Institute (TII), Абу-Даби. **Дата выпуска:** май 2023 (Falcon-40B и 7B), ноябрь 2023 (Falcon-180B).
**Размеры модели:** 7B, 40B, 180B параметров (все – dense). Также доступны fine-tuned версии: Falcon-7B-Instruct, Falcon-40B-Instruct (для задач инструкций).
**Архитектура:** Falcon основан на архитектуре трансформера от модели Google PaLM ￼, с рядом оптимизаций для эффективности инференса. Ключевое отличие – использование Multi-Query Attention (MQA): в отличие от стандартного multi-head внимания, Falcon для всех $h$ голов внимания хранит один общий ключ и значение ￼ (т.е. $n_{kv}=1$). Это существенно уменьшает объём KV-кэша и ускоряет генерацию, почти не влияя на качество ￼. Например, Falcon-40B имеет 64 головы внимания, но вместо хранения 64 отдельных ключевых/значенческих проекций хранит лишь одну – экономия 64× по памяти. Falcon также применяет ротари-позиционные эмбеддинги (RoPE) для задания позиций ￼, отказавшись от Absolute/ALiBi в окончательной версии. Нормализация – LayerNorm (с эпсилон 1e-5) до и после слоя внимания (модель реализована с Parallel Attention – параллельным применением residual connections, требующим двух LayerNorm на блок). Активация в FFN – стандартная GeLU (не стала вводиться SwiGLU из-за увеличения памяти под веса) ￼. Размер внутреннего FFN ~4× от скрытого. Например, Falcon-7B: 32 слоя, скрытая размерность ~4544, число голов 71 (каждая ~64-d), контекст 2048 токенов. Falcon-40B: 60 слоёв, скрытая ~8192, 128 голов (или групп KV, с MQA), контекст 2048. В Falcon убраны некоторые неэффективные элементы архитектуры GPT-3: нет токенов-дополнителей (совпадает BOS=EOS=1 ID), нет позиционных токенов, обучаемых эмбеддингов позиций тоже нет (только RoPE). Эмбеддинги токенов и матрица вывода (output logits) разделяют веса (weight tying) для уменьшения памяти ￼. Также отсутствуют bias-смещения в линейных слоях. В совокупности, Falcon получился высокооптимизированным: «архитектура Falcon оптимизирована под инференс, включает multi-query attention и поддержку FlashAttention» ￼.

**Предобучение:** Обучение Falcon-40B проводилось на 1 трлн токенов качественного текста (корпус RefinedWeb ~40%, дополненный кодом, общением, книгами и пр.). Falcon-7B обучен на 500 млрд токенов (подмножество того же корпуса). Falcon-180B – на 3.5 трлн токенов (один из крупнейших открытых обучающих сетов) ￼. Для обучения Falcon-40B использовалось 384 GPU A100 40GB ~2 месяца; Falcon-180B – 4096 A100 80GB ~2.5 месяца ￼. Все модели обучены с mixed precision (BF16/FP16). Лицензия выпускa Falcon-7B/40B – Apache 2.0 (доступны для коммерческого использования). В ноябре 2023 Falcon-180B также открыт (это на момент публикации одна из крупнейших открытых моделей, сопоставимая по качеству с PaLM-2 Large) ￼.

**Производительность:** Falcon-40B на момент выпуска возглавил рейтинги Open LLM по ряду метрик. В тестах он превосходит LLaMA-2 70B в zero-shot и few-shot задачах на нескольких бенчмарках, приближаясь к PaLM-2 и GPT-3.5 ￼. 7-миллиардный Falcon-7B был несколько слабее LLaMA2-7B, но за счёт MQA имел быстрее инференс.

**Fine-tuning и варианты:** TII выложил Instruct-версии Falcon (7B и 40B) – дообученные на смешанных инструкционных данных (включая ShareGPT диалоги). Они лучше подходят для пользовательских запросов (подобно ChatGPT). Кроме того, Falcon стал популярен для самостоятельного LoRA-файнтюнинга – благодаря Apache 2.0 лицензии, его можно брать за основу коммерческих приложений. Например, выпущены адаптации Falcon-7B для разговора на различных языках посредством LoRA. Для 40B модели полный fine-tuning требует значительных ресурсов (рекомендуется 8×A100 80GB или эквивалент); поэтому используются методы PEFT (LoRA/QLoRA). Falcon устойчив к 4-битному квантованию – широко доступны 4-bit и 8-bit веса для локального запуска.

**Требования к оборудованию:** Falcon-7B по требованию VRAM аналогичен LLaMA-7B (~15 ГБ FP16). Falcon-40B содержит 40 млрд параметров: в FP16 это ~80 ГБ, плюс накладные расходы – итого около 100 ГБ для инференса на 2048 токенов. Таким образом, Falcon-40B обычно разворачивают на 2–4 GPU (например, 2×A100 80GB). При 8-битном квантовании 40B модель занимает ~40 ГБ, что умещается на 1 GPU A100 80GB (или 2×RTX 3090 24GB в составе). 4-битная версия (~20 ГБ) может даже быть запущена на одной RTX 4090 (24 ГБ) с небольшим запасом. Falcon-180B – ~360 ГБ в FP16 (то есть минимум 5×80GB GPU), в 8-бит ~180 ГБ (3×80GB), и пока является больше исследовательской моделью для кластеров с поддельной поддержкой (но она открыта, и её тоже можно сжимать).

Falcon не содержит MoE-компонентов – все версии плотные. Однако исходный код позволяет расширять модель до multi-group attention (обобщение MQA), что и использовано для параллельного обучения Falcon-180B ￼. LoRA-адаптеры для Falcon работают так же, как для LLaMA.

### Mistral

**Разработчик:** Mistral AI (стартап, Франция). **Дата выпуска:** сентябрь 2023.
**Размеры модели:** Пока выпущена Mistral 7B v0.1 (7.3 млрд параметров). Заявлены планы на более крупные версии (Mistral 13B и т.д.).
**Архитектура:** Mistral 7B – модель особой оптимизированной архитектуры на 7 млрд параметров, цель которой – максимальная эффективность при небольшом размере. Базовые параметры: 32 слоя трансформера, скрытое размерность 4096, в механизме внимания 32 головы-запроса, но только 8 голов ключ/значение ￼. Это реализует подход Grouped-Query Attention (GQA): группы по 4 головы запроса разделяют одну пару K/V, что снижает затраты памяти на кэш ~в 4 раза, позволяя эффективнее работать с большим контекстом. Mistral применяет ротари-позиционное кодирование (RoPE) для внимения ￼ ￼. Контекстное окно изначально – 8192 токенов ￼, причём модель использует инновационный механизм скользящего окна внимания (Sliding Window Attention, SWA) ￼. В SWA каждый слой внимания видит лишь последние 4096 токенов контекста, а не весь 8192-контекст ￼. Однако за счёт каскадирования слоёв, высокоуровневые слои опосредованно получают информацию и из дальних токенов (токен на слое k видит окно 4096 на слое k-1, которое охватывало предыдущее окно на слое k-2 и т.д.) ￼. Таким образом, модель сохраняет эффективную память 8192, но вычислительная сложность растёт линейно O(n * 4096), а не квадратично O(n^2) ￼. Это позволило Mistral сразу поддерживать длинный контекст при умеренных затратах. К тому же, фиксированное окно 4096 токенов дало возможность хранить кэш KV ограниченного размера и обновлять его циклически (rolling buffer), экономя ~50% памяти кэша на последовательностях длины 8192 без потери качества ￼. Mistral также внедрила улучшенную реализацию FlashAttention и специализированные CUDA-ядра (совместно с разработчиками xFormers) для ускорения длинного контекста – отмечено ускорение в 2× на последовательностях 16k при окне 4k ￼. Активация в FFN – SiLU (идентично SwiGLU без разделения матрицы – фактически используется gated-linear модуль с сигмоидой и линейным слоем, как вариант SwiGLU) ￼. Размер FFN — 14336 (примерно 3.5× от скрытого) ￼, что чуть меньше стандартного 4× – очевидно, из соображений баланса качества и памяти. Нормализация – RMSNorm (как в LLaMA).

**Данные и обучение:** Mistral 7B заявлен как обученный на до 8 трлн токенов высококачественного корпуса ￼, включающего несколько языков (особо упомянут французский) и значительный объём кода. Точная разбивка данных не раскрыта, но основатели Mistral отмечали упор на качество данных и схемы curriculum learning. Обучение проводилось на облачной инфраструктуре (AWS) с ~128 GPU A100 80GB (оценочно), с использованием пиковых технологий (FlashAttention, JAX/TPU не применяли – модель обучена в PyTorch+FSDP). В результате 7B модель Mistral достигла впечатляющих результатов: по комплексным тестам (MMLU, reasoning, coding) она превосходит LLaMA2-13B и приближается к LLaMA2-34B ￼. На MMLU 7B Mistral набирает ~57%, что примерно на уровне 34-моделей предыдущего поколения. Это выдающееся соотношение качество/размер, достигаемое за счёт архитектурных улучшений (GQA, длинный контекст) и большого количества данных. Лицензия Mistral 7B – полностью открытая (Apache 2.0).

**Инференс и память:** Mistral 7B содержит 7.3 млрд параметров – немного больше, чем LLaMA2-7B, из-за дополнительных вспомогательных матриц (например, 8 KV-глав вместо 32). Но по памяти разница несущественна: ~15 ГБ в FP16. Благодаря оптимизациям, Mistral может эффективно работать с 8k контекстом там, где другие 7B модели ограничены 4k. При 8k токенах задержка инференса увеличивается не квадратично, а примерно в 2 раза по сравнению с 4k (за счёт SWA). VRAM-кэш для 8k у Mistral в 2 раза меньше, чем потребовался бы без GQA/SWA, – эквивалентен хранению ~4k токенов на полную модель ￼. Это означает, что Mistral 7B с контекстом 8k может работать на GPU 16–24 ГБ без переполнения памяти, чего трудно достичь с обычными 7B (им для 8k нужно ~двойной кэш). Mistral легко квантуется до 4 бит – доступны GPTQ-веса 4bit, которые помещаются в ~4 ГБ, позволяя запуск на потребительских GPU с 6–8 ГБ памяти (правда, с более медленным декодированием на таких видеокартах). Fine-tuning Mistral 7B также делается через LoRA: выпущена версия Mistral 7B Instruct (дообучена на инструкциях с открытых датасетов) ￼, показавшая улучшение диалоговых навыков. LoRA для Mistral 7B требует около 10 ГБ VRAM (на 8 бит адаптеры), что доступно на современных GPU.

**Особенности и развитие:** Mistral 7B демонстрирует важность архитектурных решений: будучи ровесником LLaMA2, она существенно выигрывает у LLaMA2-7B в задачах рассуждения, математики и кода ￼. Это указывает, что «архитектура всё ещё имеет значение» – слоган команды Mistral. В планах – выпуск моделей 13B и 30B с той же архитектурой. Можно ожидать, что 13B Mistral превзойдёт 34B LLaMA2. Кроме того, Mistral может масштабировать контекст сверх 8k: сообщество уже экспериментирует с расширением RoPE (методы RoPER, etc.) вплоть до 32k токенов на основе Mistral ￼. Что касается MoE: текущая Mistral 7B – плотная модель, но в блоге Mistral упоминалось про «Mistral 8x7B MoE», намекая на будущую 56B модель, составленную из 8 экспертных 7B. В официальных материалах Mistral-7B-MoE пока нет, однако архитектура вполне совместима с добавлением MoE-слоёв.

### BLOOM

**Разработчик:** Проект BigScience (международная коллаборация при Hugging Face). **Дата выпуска:** июль 2022.
**Размеры модели:** Основная модель BLOOM – 176 млрд параметров. Также выпущены уменьшенные версии: 560 млн, 1.1B, 1.7B, 3B, 7.1B параметров ￼, обученные на тех же данных (для исследовательских сравнений). Отдельно существуют модели BLOOMZ – версии BLOOM, дообученные на задачах инструкций/переводов (мультиязыковой аналог InstructGPT).
**Архитектура:** BLOOM – авто-регрессивный трансформер, по строению близкий к GPT-3 ￼. В архитектуре практически повторён GPT-3 175B, за исключением двух изменений: (1) применена схема позиционного кодирования ALiBi (Attention with Linear Bias) вместо абсолютных позиций, и (2) использована обратная порядок Residual-механизма (Post-norm vs Pre-norm). Согласно техническому отчёту, в BLOOM решили отказаться от явных позиционных эмбеддингов и добавления позиции к входу; вместо этого в механизм внимания встроен линейный байас, увеличивающий приоритет ближних токенов линейно с увеличением расстояния (ALiBi) ￼. ALiBi позволяет модели обобщать на контексты длиннее обученных без дообучения (принцип "Train short, test long"). Обучающий контекст BLOOM – 2048 токенов, но благодаря ALiBi можно проводить инференс на более длинных последовательностях (первые эксперименты показывали работоспособность до ~3k-4k). Второе отличие: резидуальные связи применяются до LayerNorm (PreNorm), тогда как в GPT-3 нормализация шла после суммирования (в BLOOM apply_residual_connection_post_layernorm=False ￼). Эта деталь влияет на устойчивость при глубоком слое. В BLOOM 176B – 70 слоёв трансформера, скрытая размерность 14336, число голов внимания 112, размер головы 128 (таким образом, 112*128 = 14336). Внутренний размер FFN – 4× скрытого (≈ 57344). Нормализация – LayerNorm с эпсилон 1e-5. Активация – GeLU. В начальной конфигурации BLOOM рассматривался вариант с ротари-эмбеддингами (как в GPT-NeoX) ￼, но в финальной версии выбрали ALiBi из-за удобства масштабирования контекста ￼. В отличие от LLaMA, в BLOOM сохранили классическое многоголовое внимание (без MQA/GQA): 112 голов (ключи/значения тоже 112). Поэтому требования к памяти у BLOOM огромны.

**Предобучение:** BLOOM 176B – первая модель такого масштаба, открыто обученная не в BigTech-компании. Обучение заняло 3.5 месяца на 384 GPU Nvidia A100 80GB ￼. Использовался платформенный стек Megatron-DeepSpeed для распределения по данным, тензорам и конвейеру (3D parallelism) ￼. Для оптимизации памяти применили ZeRO-Infinity и CPU Offloading. Веса хранились в BF16, оптимизатор – Adam с параметрами, аналогичными GPT-3 (β₂=0.95, LR ~2e-5, Warmup 2% итераций). Датасет: 350 млрд токенов на 46 натуральных языках и 13 языках программирования ￼. В отличие от LLaMA, целью BLOOM было многоязычие: <30% обучающих данных – английский. Крупнейшие компоненты: англ. RefinedWeb, франц. тексты, испанский, арабский, русский, код (Python, JS, C, etc.), догенерированные переводы, и т.д. Общий объём сырого текста – 1.61 ТБ. Распределение языков: англ ~30%, франц ~17%, араб ~13%, испан ~13%, затем низкоресурсные европейские, африканские и др. (вплоть до каталонского, суахили, украинского и т.д. по ~1-2%). Такой разнообразный корпус сделал BLOOM первой глобальной LLM.

**Лицензия и доступ:** BLOOM выпущен под лицензией RAIL (Responsible AI License) – открытой, но с оговорками об этичном использовании. Веса 176B доступны через HuggingFace (отдельно по 8 shards ~50ГБ каждая). Модели до 7.1B – в виде обычных файлов. Также опубликован подробный отчёт (≈100 стр) с описанием процесса. BLOOM стал базой для множества исследований, включая BLOOMZ – его дообучение на задачах NLU с учителем (cross-lingual transfer, multitask fine-tuning на английских инструкциях с последующим переводом). BLOOMZ и mBLOOM сразу показали, что BLOOM можно превратить в полезную чат-модель.

**Инференс и требования:** Полная 176B модель крайне требовательна: только веса FP16 занимают ~352 ГБ памяти. В реальном инференсе с учетом активаций требуется ~>400 ГБ (при контексте 2048) – то есть минимум 6× GPU A100 80GB (5×80 = 400, но нужен запас) или 8× 48GB с хорошей шиной. Однако, как отмечалось в исследовании, 8-битное представление позволяет разместить BLOOM 176B на 1 сервере с 8×A100 40GB ￼. А с 4-битным GPU-квантованием модель можно вписать в ~88 ГБ, что «почти помещается на одном 80ГБ GPU» ￼ ￼ (то есть 176B в 4-bit = 88 ГБ – к сожалению, чуть выше 80ГБ, но на 2×48GB уже возможно). Для большинства практических целей используют версию BLOOM 7B1 – 7.1 млрд параметров, она качеством близка к GPT-3 6.7B и может запускаться на одном среднем GPU (16 ГБ). Также активно применяют BLOOM 176B через API inference-инстансов на HuggingFace или через децентрализованные подходы (Petals – распределённый инференс по интернету).

**Особенности и fine-tuning:** BLOOM показал ценность многоязычного обучения – он лучше GPT-3 справляется с задачами на французском, арабском и других языках ￼. Однако на английском он несколько уступает сопоставимым по размеру (176B) моделям, вероятно из-за потери части капацитета на многоязычность. Тем не менее, дообучение BLOOMZ сделало его полезным для инструкций: BLOOMZ-176B почти догоняет GPT-3.5 в некоторых benchmarks. Есть также BLOOM-Chat (инструкционная fine-tune). BLOOM поддерживает LoRA-дообучение: например, команда HuggingFace показывала, что 176B можно fine-tune с Low-Rank адаптерами на ~8×A100 80GB (что куда менее затратно, чем полный fine-tune, требующий >16 GPUs). Также BLOOM применялся как тест для 8-bit оптимизаторов (библиотека bitsandbytes от Тима Деттмерса). В рамках BigScience были эксперименты по разреженным моделям: например, выпущена SparseGPT версия BLOOM (контролируемое обнуление части весов без сильного падения качества), а также производился анализ вырожденных нейронов (которые можно убрать). MoE-версии непосредственно не делали, но архитектура позволяет встроить MoE слои (Megatron-LM имеет такую возможность).

Подводя итог, BLOOM стал значимым шагом к открытому сообществу LLM – он доказал, что даже Hundred-Billion+ модель можно обучить относительно небольшими силами при международной кооперации и открыто поделиться результатом.

### GPT-Neo, GPT-J и GPT-NeoX (EleutherAI)

**Разработчик:** EleutherAI (независимая исследовательская группа). **Выпуск:** 2021–2022.
**Модели:** GPT-Neo (1.3B и 2.7B параметров, выпущены в марте 2021), GPT-J-6B (6 млрд, июнь 2021), GPT-NeoX-20B (20 млрд, февраль 2022). Также семейство Pythia (2023) – набор из 16 моделей от 70M до 12B, обученных на одинаковых данных для изучения масштабируемости. Здесь рассмотрим основные: GPT-J-6B и GPT-NeoX-20B, как предшественники современных открытых LLM.

**Архитектура:** Это генеративные трансформеры GPT-подобного типа (decoder-only), спроектированные по образцу оригинального GPT-3. GPT-J-6B имеет 28 слоёв, скрытая размерность 4096, 16 голов внимания, контекст 2048 токенов ￼. Применяются ротари-позиционные эмбеддинги (RoPE) на 64 измерениях из 256 в каждой голове ￼ – GPT-J стал одной из первых моделей, внедривших RoPE (вместо абсолютных позиций GPT-2). Нормализация – LayerNorm (две LN на блок: перед вниманием и перед FFN). Активация – GeLU. GPT-J включал bias в линейных слоях, как в GPT-3 (в отличие от LLaMA, убравшей bias). GPT-NeoX-20B – более масштабная: 44 слоя, скрытая ~6144, 64 головы (каждая 96-мерная), тоже контекст 2048 ￼. Она также использует RoPE и архитектурно очень близка к GPT-J (чем и отличается от, скажем, FairSeq-13B от Meta 2021 года, где были абсолютные эмбеддинги). В GPT-NeoX кодовая база (Megatron-LM fork) поддерживала и ALiBi, но в релизной 20B модели остался RoPE. Обе модели – плотные (dense), без MQA/GQA (многоголовое внимание стандартное).

**Обучение:** GPT-Neo и GPT-J были обучены на датасете Pile (825 ГБ текстов, ~300 млрд токенов) – сборнике разнообразных источников (интернет-форумы, новостные статьи, литература, GitHub, ArXiv, т.д.). GPT-J-6B обучалась с Mesh TensorFlow на TPU Pod (примерно 256 TPUv3) в течение ~5 дней. GPT-NeoX-20B обучалась на 96 GPU A100 40GB ~ по 150к шагов, батч ~3 млн токенов ￼ (суммарно ~450 млрд токенов, то есть ~1.5 прохода по Pile). Оптимизатор – AdamW (β₂=0.95). LR ~1e-4 с косинусным спадом. Формат – FP16 с динамическим лосс-скейлингом. Эти модели послужили «первой волной» открытых LLM, предоставив сообществу аналоги GPT-3 (пусть и меньшего масштаба). Лицензии – Apache 2.0 (все веса свободно доступны).

**Производительность:** GPT-J-6B на момент выхода превосходил тогдашние аналогичные по размеру модели (Eleuther сравнивал с FairSeq-13B и GPT-neo 2.7B). Он стал популярным благодаря балансy качества и размера – 6B модель можно запускать на одной GPU 16GB. GPT-NeoX-20B до появления LLaMA была крупнейшей открытой англоязычной LLM. Она показывала результаты, близкие к GPT-3 13B на многих задачах, и в некоторых превзошла GPT-3 175B в few-shot режиме ￼ (как отмечалось EleutherAI, 20B модель особенно сильна в пяти-шаговых задачах по сравнению с аналогами). Тем не менее, NeoX-20B уступает более новым LLaMA2-13B и Falcon-40B по совокупности тестов, что ожидаемо ввиду прогресса.

**Инференс и требования:** GPT-J-6B (6 млрд) – одна из самых доступных: FP16 ~12 ГБ, INT8 ~6 ГБ. Поэтому она широко использовалась в embedded-средах (например, Cerebras выпускала ее на своей железе). Для 6B модели достаточно одной RTX 3090. GPT-NeoX-20B – ~40 ГБ FP16, то есть минимум две 24-GB GPU или одна A100 40GB. В 8-битном формате – ~20 ГБ, что впритык помещается на RTX 3090/4090. 20B модель может обслуживаться и на CPU с оптимизированными библиотеками (например, с помощью quantization и многопоточности, но скорость будет низкой). Контекст 2048 – стандартный, однако проекты вроде GPT-NeoX-20B-Long выпустили перераспределённые веса с контекстом 4096/8192 (с ALiBi, например) для расширения возможностей. LoRA/QLoRA для этих моделей также применимы. Например, Databricks Dolly 2.0 – это GPT-NeoX-12B (Pythia-12B) с LoRA-дообучением на инструкциях, показавший приличное качество ответа при всего 12B параметрах. Vicuna-13B (на LLaMA) всё же превзошла Dolly, но Dolly была полностью свободна и коммерчески пригодна, что важно.

**Особенности и дальнейшее:** Модели EleutherAI открыли дорогу множеству приложений и исследований. Их архитектура легла в основу последующих: фактически LLaMA и Falcon заимствовали идеи Rotary embeddings и pre-normalization, впервые опробованные в GPT-J/NeoX. Проект Pythia (Eleuther, 2023) предоставил набор моделей 70M–12B, обученных на единообразном корпусе Pile-v2 с разным количеством токенов, что позволило исследователям изучать влияние масштаба. На основе Pythia-12B была создана коммерчески открытая модель StableLM 7B/15B и Dolly 2.0. Хотя сейчас LLaMA2 и Mistral несколько потеснили NeoX по качеству, 20B остаётся наибольшей моделью под полностью открытой лицензией Apache среди англоязычных. С точки зрения поддержки: NeoX-20B поддерживает int8 матрицы (есть оптим. версия для FasterTransformer), LoRA fine-tune (через PEFT). MoE вариантов не было публично, но EleutherAI экспериментировал с Switch-Transformers в ранних фазах.

### Другие заметные открытые модели

Наряду с вышеперечисленными, существует ряд других открытых LLM, заслуживающих упоминания:
	•	OPT (Meta, 2022): семейство моделей (125M – 175B параметров) выпущенное для исследования. OPT-175B (175 млрд) – открытый аналог GPT-3, выложенный под несвободной лицензией (некоммерческой), но доступный по запросу. Архитектурно OPT повторяет GPT-3 (LayerNorm с пост-резид. применением, Learned positional embeddings, контекст 2048). OPT-175B обучена на 180 млрд токенов (MDI, ThePile и др.) с фазовым расписанием LR. Хотя качество OPT-175B заметно ниже GPT-3, её выпуск был важным шагом. Для большинства разработчиков более актуальны OPT-13B и OPT-30B, которые доступны сразу (30B модель – на уровне GPT-3 13B). OPT заложила основу, на которой позже улучшили LLaMA.
	•	GLM-130B (Tsinghua Univ, 2022): 130 млрд параметров, двуязычная (английский и китайский) модель. Она примечательна тем, что основана на архитектуре GLM (General Language Model), объединяющей autoregressive и маскированное обучение. GLM-130B обучена на 400 млрд токенов (соотношение 1:1 англ-китайский). Архитектура – трансформер с ALiBi, контекст 2048, но обучен в режиме унифицированного заполнения пропусков (UniLM). GLM-130B достигла качества GPT-3.5 в китайском языке и близка к нему в английском. Веса модели доступны по запросу (лицензия Apache 2.0). Однако инференс требователен (260 ГБ в FP16 ￼). Проект ChatGLM развивает эту линию, выпуская 6B и 2B модели для китайского.
	•	MPT-7B (MosaicML, 2023): 7 млрд параметров, обученная на 1 трлн токенов (данные Mosaic). Уникальна поддержкой большого контекста – 65k токенов – через метод ALiBi. MPT-7B-StoryWriter с контекстом 65k хорошо подходит для работы с длинными документами. Лицензия Apache 2.0. Также Mosaic выпустила MPT-30B (30 млрд) – более качественную модель, но из-за лицензии (CC BY-NC) она полу-открытая. MPT-7B применяет FlashAttention и можно легко fine-tune с LoRA.
	•	Baichuan-13B (2023, Китай): 13 млрд, аналог LLaMA, обученный на китайско-английском корпусе. Позиционируется как открытая (Apache 2.0) для коммерческого использования, что важно для китайского рынка. Качество на китайском близко к ChatGPT. Arch – как LLaMA (RoPE, SwiGLU, RMSNorm). Есть Baichuan-7B, Baichuan-13B и их чат-версии.
	•	Phi-1, Phi-2, Phi-3 (Microsoft, 2023-24): серия «малых» LLM (Small Language Models) с упором на эффективность. Phi-1 (1.3B) и Phi-2 (2.7B) были внутренними, затем открыты Phi-3 модели: 3.8B, 7B, 14B. Они обучены на значительно урезанном словаре (как будто «язык 4-летнего ребёнка» – идея авторов) и огромном количестве токенов. Phi-3.8B обучена на 3.4 трлн токенов (!) за 10 дней на 512 GPU H100 ￼, что беспрецедентно высокий таргет по data-to-parameters. Это дало Phi-3-mini (3.8B) качество, превосходящее LLaMA2 8B и Mistral7B на многих тестах ￼. Phi-3-small (7B) и 3-medium (14B) ожидаются. В августе 2024 объявлены Phi-3.5 модели: Phi-3.5-MoE-instruct (42B) – смесь экспертов, Phi-3.5-mini-instruct (3.8B) – базовая, и Phi-3.5-vision – мультимодальная. MoE вариант (41.9B) состоит из нескольких экспертов и обучен на 4.9 трлн токенов за 23 дня на 512 H100 ￼ ￼. Все Phi-модели открыты (MIT License) и доступны в Azure Model Catalog и HuggingFace. Они демонстрируют новый подход: не увеличивать модель до сотен миллиардов, а максимально обучить меньшие модели, и при необходимости использовать MoE для роста параметров. Это перспективно для внедрения на устройствах (edge). Аппаратные требования Phi-3.5-mini 3.8B минимальны – она может работать даже на GPU с 4 ГБ (8-битный инференс <4ГБ). Phi-3.5-MoE 42B требует уже >80 ГБ (из-за экспертов).

### Сводная таблица характеристик моделей:

| Модель               | Парам.         | Контекст        | Архитектура и механизмы            | Особенности тренировки             | Лицензия            |
|----------------------|----------------|-----------------|------------------------------------|------------------------------------|---------------------|
| LLaMA 1 (Meta)       | 7B,13B,33B,65B | 2048 (L2: 4096) | RMSNorm, SwiGLU, MQA, RoPE ￼       | 1T–1.4T токенов, AdamW ￼, A100 cluster | Non-com (L2: перм.) |
| Falcon (TII)         | 7B,40B,180B    | 2048            | LayerNorm, GeLU, MQA, RoPE ￼ ￼     | RefinedWeb 1T–3.5T ток., AWS P4d ￼ | Apache 2.0          |
| Mistral 7B           | 7.3B           | 8192            | RMSNorm, SiLU, GQA, RoPE, SWA ￼ ￼  | до 8T токенов, FlashAttn, xFormers ￼ | Apache 2.0          |
| BLOOM (BigScience)   | 176B (и 7B…)   | 2048            | LayerNorm, GeLU, Multi-head, ALiBi ￼ | 350B токенов на 46 языках ￼, BF16 ￼ | RAIL (открыт)       |
| GPT-J (Eleuther)     | 6B             | 2048            | LayerNorm, GeLU, Multi-head, RoPE ￼  | 300B токенов (Pile), TPU-v3 Mesh   | Apache 2.0          |
| GPT-NeoX (Eleuther)  | 20B            | 2048            | LayerNorm, GeLU, Multi-head, RoPE | 400B+ токенов (Pile), A100-40GB x96 ￼ | Apache 2.0          |
| OPT (Meta)           | 175B (и <)     | 2048            | LayerNorm post, GeLU, MH, Learned Pos. | 180B ток., декарбон. чекп.        | Non-com             |
| GLM-130B (THU)       | 130B           | 2048            | LayerNorm, GELU, MH, ALiBi         | 400B ток (EN/ZH), UniLM (mix AR+MLM) | Apache 2.0*         |
| MPT-7B (Mosaic)      | 7B             | 2048 (Story:65k)| Norm, GELU, MH, ALiBi              | 1T токенов, FlashAttn, FSDP        | Apache 2.0          |
| Baichuan-13B         | 13B            | 2048            | RMSNorm, SwiGLU, RoPE (LLaMA)      | ~1T ток (zh/en/code)              | Apache 2.0          |
| Phi-3 (MSR)          | 3.8B,7B,14B    | 2048            | RMSNorm, GELU, MH, RoPE            | 3.4T–8T токенов (!), 512×H100 ￼     | MIT                 |
| Phi-3.5-MoE        | 42B (MoE)      | 2048            | MoE (8 эксперт.) + MH              | 4.9T токенов, 512×H100 (23 дня) ￼ ￼ | MIT                 |

(Примечание: Лицензия Apache 2.0 указана, если модель доступна для коммерческого использования; «Non-com» – только научная некоммерческая; RAIL – открытая с условиями; MIT – максимально свободная.)

### Требования к оборудованию: обзор

Для ясности сопоставим приблизительные требования VRAM для разных размеров моделей (без учёта квантования):
	•	1 млрд параметров ≈ 2 ГБ памяти (FP16). Например, 6B ~12ГБ, 7B ~14ГБ, 13B ~26ГБ, 20B ~40ГБ, 30B ~60ГБ, 40B ~80ГБ, 65-70B ~130ГБ, 175B ~350ГБ (веса). В реальном инференсе требуется ~20–30% дополнительно (под активации, кэши и пр.). Например, для 130B оценивалось ~320 ГБ нужно с учётом активаций ￼.
	•	Благодаря 8-битному представлению, потребление сокращается вдвое: та же 130B модель ~160 ГБ, а 175B ~180 ГБ, что делает возможным их запуск на кластере из 3–4 GPU 80GB ￼. 4-битное сокращает ещё в 2 раза: ~88 ГБ для 130B ￼, ~90 ГБ для 176B – уже ближе к реальности одного сервера с 8 GPU 24GB.
	•	Практические примеры: LLaMA2-70B (70 млрд) – для базового FP16 инференса нужен как минимум 2×80GB (либо 4×40GB). В 8-бит можно уложить на 1×80GB (70B≈35GB 8-bit + накладные ~45GB = 80GB). В 4-бит 70B ≈ 18GB, что даже на одну 24GB GPU помещается (оставляя место под большие последовательности). Поэтому методы QLoRA смогли fine-tune 65B на одной 48GB, загрузив модель в 4-бит.
	•	TPU vs GPU: Большинство открытых моделей обучены на GPU (NVIDIA). Исключения – OpenLLaMA (TPU-v4 Google), GPT-J (TPU v3), GLM-130B (самописная инфраструктура на суперкомпьютере с CPU + GPU миксом). Для инференса TPU не используются широко вне GCP. Для развертывания лучше ориентироваться на GPU.

**Стек для разработки:** Открытые модели поставляются обычно в формате HuggingFace Transformers (pytorch), что упрощает их использование. Для ускорения инференса применяют библиотеки: HF Accelerate, DeepSpeed (Zero-Inference), FasterTransformer (оптимизированные CUDA ядра), vLLM (эффективное управление KV-кэшем) и др. Также получили развитие фреймворки для сжатия: Distillation (например, модель LLaMA-2 70B -> 7B через distill), прунинг (SparseGPT, Wanda). Всё это активно исследуется.

Заключение

Открытая экосистема LLM стремительно развивается. Если в 2021 году максимальной открытой моделью была 6-миллиардная GPT-J, то к 2025 году у сообщества есть уже несколько моделей сверх 100 млрд параметров (Falcon-180B, GLM-130B, BLOOM-176B, LLaMA2-70B), доступных для исследования и частично для коммерческого использования. Технически прогресс выражается не только в росте параметров, но и в улучшении архитектуры – новые методы внимания (MQA, GQA, SWA), улучшенные функции активации (SwiGLU), эффективные позиционные кодирования (RoPE, ALiBi), а также в подходах к эффективному обучению (как в Phi-series, Mistral). Одновременно развиваются методы оптимизации инференса: смешанная точность, квантование, эвристики кэширования и пр., что делает возможным внедрение LLM вне облака – на локальных машинах и даже на мобильных устройствах. С поддержкой сообществом инициатив вроде HuggingFace, EleutherAI, BigScience и новых компаний (Mistral, MosaicML), можно ожидать дальнейшего появления открытых моделей, приближающихся по возможностям к проприетарным системам. Этот справочник охватывает состояние на начало 2025 года – читателю рекомендуется следить за обновлениями, так как поле LLM развивается очень динамично.

Источники и ссылки: [Meta AI LLaMA paper & card ￼], [HuggingFace BLOOM blog ￼ ￼], [TII Falcon 180B arXiv ￼ ￼], [Mistral AI announcement ￼ ￼], [Microsoft Phi news ￼ ￼], [EleutherAI GPT-J card ￼ ￼], [Dettmers et al. on quantization ￼], и др.

## Структура проекта

```
├── public/              # Статические файлы
├── src/
│   ├── components/        # React компоненты
│   │   ├── Calculator/    # Компоненты калькулятора (Settings, Results, Report, Analytics)
│   │   └── UI/            # Переиспользуемые UI компоненты (сейчас пустое)
│   ├── data/              # Пресеты (модели, GPU, серверы)
│   ├── hooks/             # Кастомные хуки (useCalculator)
│   ├── utils/             # Утилиты (расчеты, валидация)
│   ├── App.jsx            # Корневой компонент приложения
│   └── index.css          # Глобальные стили (Ant Design import)
├── index.html           # HTML точка входа для Vite
├── index.jsx            # Точка входа React приложения
├── package.json         # Зависимости и скрипты проекта
├── vite.config.js       # Конфигурация Vite
└── README.md            # Этот файл
```

## Лицензия

Проект распространяется под лицензией ISC (см. `package.json`). 